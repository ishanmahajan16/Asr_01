{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from sklearn.mixture.gaussian_mixture import _compute_precision_cholesky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform(file1,file2):\n",
    "    x=np.load(file1).tolist()\n",
    "    y=np.load(file2).tolist()\n",
    "    return pd.DataFrame({\"feature\":x,\"label\":y})\n",
    "\n",
    "\n",
    "pathf=[\"features/mfcc_test/features_test.npy\",\"features/mfcc_test_delta/features_test_delta.npy\",\"features/mfcc_test_delta_delta/features_test_delta_delta.npy\"]\n",
    "pathl=[\"features/mfcc_test/labels_test.npy\",\"features/mfcc_test_delta/labels_test_delta.npy\",\"features/mfcc_test_delta_delta/labels_test_delta_delta.npy\"]\n",
    "df = [transform(pathf[0],pathl[0]),transform(pathf[1],pathl[1]),transform(pathf[2],pathl[2])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "delta_with=[\"./models/mfcc/with_energy_coeff/\",\"./models/mfcc_delta/with_energy_coeff/\",\"./models/mfcc_delta_delta/with_energy_coeff/\"]\n",
    "delta_without=[\"./models/mfcc/without_energy_coeff/\",\"./models/mfcc_delta/without_energy_coeff/\",\"./models/mfcc_delta_delta/without_energy_coeff/\"]\n",
    "dl=[[0],[0,14],[0,14,27]]\n",
    "\n",
    "ph_score_with=[]\n",
    "ph_score_without=[]\n",
    "ph_score_comp=[]\n",
    "\n",
    "for j in range(0,3):\n",
    "    print(j)\n",
    "    \n",
    "    ph_score_with.append([])\n",
    "    ph_score_without.append([])\n",
    "    ph_score_comp.append([])\n",
    "    phoneme=df[j]['label'].unique()\n",
    "    phoneme.sort()\n",
    "    \n",
    "    for ph in phoneme:\n",
    "        print(ph)\n",
    "        temp=df[j]\n",
    "        features=np.array(temp[\"feature\"].tolist())\n",
    "        #labels.append(np.array(temp[\"label\"].tolist()))\n",
    "        \n",
    "        models=pickle.load(open(delta_with[j]+'/64/'+ph+\".pkl\", 'rb'))\n",
    "        ph_score_with[j].append(models.score_samples(features).tolist())\n",
    "        \n",
    "        features=np.delete(features,dl[j], axis=1)\n",
    "        \n",
    "        models=pickle.load(open(delta_without[j]+'/64/'+ph+\".pkl\", 'rb'))\n",
    "        ph_score_without[j].append(models.score_samples(features).tolist())\n",
    "       \n",
    "        \n",
    "        if j==0:\n",
    "            \n",
    "            comp=[2,4,8,16,32,128,256]\n",
    "            ph_score_comp=np.empty((7),dtype=[])\n",
    "            \n",
    "            ph_score_comp=[]\n",
    "            for i in comp:\n",
    "                ph_score_comp.append([])\n",
    "            \n",
    "            \n",
    "            count=0\n",
    "            for i in comp:\n",
    "                \n",
    "                print(i)\n",
    "                \n",
    "                models=pickle.load(open(delta_without[j]+'/'+(str)(i)+\"/\"+ph+\".pkl\", 'rb'))\n",
    "                ph_score_comp[count].append(models.score_samples(features).tolist())\n",
    "                count=count+1\n",
    "                \n",
    "        \n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making phoneme dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, 'aa': 1, 'ae': 2, 'ah': 3, 'aw': 4, 'ay': 5, 'b': 6, 'ch': 7, 'd': 8, 'dh': 9, 'dx': 10, 'eh': 11, 'er': 12, 'ey': 13, 'f': 14, 'g': 15, 'hh': 16, 'ih': 17, 'iy': 18, 'jh': 19, 'k': 20, 'l': 21, 'm': 22, 'n': 23, 'ng': 24, 'ow': 25, 'oy': 26, 'p': 27, 'r': 28, 's': 29, 'sh': 30, 'sil': 31, 't': 32, 'th': 33, 'uh': 34, 'uw': 35, 'v': 36, 'w': 37, 'y': 38, 'z': 39}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "phoneme=df[0]['label'].unique()\n",
    "phoneme.sort()\n",
    "\n",
    "ph={v: k for k, v in dict(enumerate(phoneme)).items()}\n",
    "\n",
    "print(ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding phoneme to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=np.array(df[0][\"label\"].tolist())\n",
    "labels=np.array(list(map(lambda x:ph[x],labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component: 64   no_delta  with_energy_coefficient   accuracy:  0.14409954390470708\n",
      "component: 64   no_delta  without_energy_coefficient   accuracy:  0.1329318513926405\n",
      "component: 64   delta+1  with_energy_coefficient   accuracy:  0.18861090200593367\n",
      "component: 64   delta+1  without_energy_coefficient   accuracy:  0.17534207146968958\n",
      "component: 64   delta+2  with_energy_coefficient   accuracy:  0.19261391312048887\n",
      "component: 64   delta+2  without_energy_coefficient   accuracy:  0.17387636717885135\n",
      "component:  2  no_delta   without_energy_coefficient   accuracy:  0.024009210468051188\n",
      "component:  4  no_delta   without_energy_coefficient   accuracy:  0.024009210468051188\n",
      "component:  8  no_delta   without_energy_coefficient   accuracy:  0.024009210468051188\n",
      "component:  16  no_delta   without_energy_coefficient   accuracy:  0.024009210468051188\n",
      "component:  32  no_delta   without_energy_coefficient   accuracy:  0.024009210468051188\n",
      "component:  128  no_delta   without_energy_coefficient   accuracy:  0.024009210468051188\n",
      "component:  256  no_delta   without_energy_coefficient   accuracy:  0.024009210468051188\n"
     ]
    }
   ],
   "source": [
    "delta=[\"no_delta\",\"delta+1\",\"delta+2\"]\n",
    "for j in range(0,3):\n",
    "    predicted=np.argmax(ph_score_with[j],axis=0).reshape(-1)\n",
    "    accuracy=np.where(predicted==labels)[0].shape[0]/labels.shape[0]\n",
    "    print(\"component: 64\",\" \",delta[j] ,\" with_energy_coefficient \",\" accuracy: \",accuracy)\n",
    "    \n",
    "    predicted=np.argmax(ph_score_without[j],axis=0).reshape(-1)\n",
    "    accuracy=np.where(predicted==labels)[0].shape[0]/labels.shape[0]\n",
    "    print(\"component: 64\",\" \",delta[j] ,\" without_energy_coefficient \",\" accuracy: \",accuracy)\n",
    "    \n",
    "comp=[2,4,8,16,32,128,256]\n",
    "for i in range(len(comp)):\n",
    "    predicted=np.argmax(ph_score_comp[i],axis=0).reshape(-1)\n",
    "    accuracy=np.where(predicted==labels)[0].shape[0]/labels.shape[0]\n",
    "    print(\"component: \",comp[i],\" no_delta \",\" without_energy_coefficient \",\" accuracy: \",accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat=[np.load(\"features_test.npy\"),np.load(\"features_test_delta.npy\"),np.load(\"features_test_delta_delta.npy\")]\n",
    "lab=[np.load(\"labels_test.npy\"),np.load(\"labels_test_delta.npy\"),np.load(\"labels_test_delta_delta.npy\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing ground truth and predicted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_with=[\"D:\\deeplearning\\Examples\\ASR_2018_T01-master\\p1\\TEST_PER/mfcc/with_energy_coeff/\",\"./TEST_PER/mfcc_delta/with_energy_coeff/\",\"./TEST_PER/mfcc_delta_delta/with_energy_coeff/\"]\n",
    "delta_without=[\"D:\\deeplearning\\Examples\\ASR_2018_T01-master\\p1\\TEST_PER/mfcc/without_energy_coeff/\",\"./TEST_PER/mfcc_delta/without_energy_coeff/\",\"./TEST_PER/mfcc_delta_delta/without_energy_coeff/\"]\n",
    "\n",
    "sentences_with=[]\n",
    "sentences_without=[]\n",
    "sentences_comp=[]\n",
    "for j in range(3):\n",
    "    sentences_with.append(\"\")\n",
    "    sentences_without.append(\"\")\n",
    "for i in range(7):\n",
    "    sentences_comp.append(\"\")\n",
    "\n",
    "sentences_ground=\"\"\n",
    "sm=0\n",
    "for i in range(feat[j].shape[0]):\n",
    "    print(i)\n",
    "        \n",
    "    for j in range(0,3):\n",
    "        \n",
    "        \n",
    "        \n",
    "        tp=np.array(ph_score_with[j])\n",
    "        \n",
    "        sentences_with[j]= sentences_with[j]+np.array2string(np.argmax(tp[:,sm:sm+feat[j][i].shape[0]],axis=0),max_line_width=2000)+\"\\r\\n\"\n",
    "        \n",
    "        tp=np.array(ph_score_without[j])\n",
    "        sentences_without[j]=sentences_without[j]+np.array2string(np.argmax(tp[:,sm:sm+feat[j][i].shape[0]],axis=0),max_line_width=2000)+\"\\r\\n\"\n",
    "        \n",
    "    \n",
    "    sentences_ground=sentences_ground+np.array2string(labels[sm:sm+feat[j][i].shape[0]],max_line_width=30000)+\"\\n\"\n",
    "      \n",
    "    for j in range(7):\n",
    "        tp=np.array(ph_score_comp[j])\n",
    "        sentences_comp[j]=sentences_comp[j]+np.array2string(np.argmax(tp[:,sm:sm+feat[0][i].shape[0]],axis=0),max_line_width=2000)+\"\\r\\n\"\n",
    "        \n",
    "    sm=sm+feat[0][i].shape[0]\n",
    "            \n",
    "        #print(feat[j][i].shape[0])\n",
    "        #input()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumping ground truth and predicted on TEST PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"D:/deeplearning/Examples/ASR_2018_T01-master/p1/\"\n",
    "delta_with=[path+\"TEST_PER/mfcc/with_energy_coeff/\",path+\"TEST_PER/mfcc_delta/with_energy_coeff/\",path+\"TEST_PER/mfcc_delta_delta/with_energy_coeff/\"]\n",
    "delta_without=[path+\"TEST_PER/mfcc/without_energy_coeff/\",path+\"TEST_PER/mfcc_delta/without_energy_coeff/\",path+\"/TEST_PER/mfcc_delta_delta/without_energy_coeff/\"]\n",
    "\n",
    "with open(path+\"TEST_PER/mfcc_ground/label.txt\",\"w\") as f:\n",
    "    f.write(str(sentences_ground))\n",
    "\n",
    "for j in range(3):\n",
    "    \n",
    "    f=open(delta_with[j]+\"_64.txt\",\"w\")\n",
    "    f.write(sentences_with[j])\n",
    "    f.close()\n",
    "    \n",
    "    f=open(delta_without[j]+\"_64.txt\",\"w\")\n",
    "    f.write(sentences_without[j])\n",
    "    f.close()\n",
    "    \n",
    " \n",
    "    \n",
    "comp=[2,4,8,16,32,128,256]\n",
    "for i in range(7):\n",
    "    f=open(delta_without[0]+\"_\"+str(comp[i])+\".txt\",\"w\")\n",
    "    f.write(sentences_comp[i]+\"\")\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
